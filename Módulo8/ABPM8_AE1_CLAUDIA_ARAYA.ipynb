{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**¿Qué es Tensorflow?**"
      ],
      "metadata": {
        "id": "LCWYl5uYY0f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "TensorFlow es una plataforma de código abierto desarrollada por Google para realizar tareas de aprendizaje automático y desarrollo de modelos de inteligencia artificial. Ofrece una amplia variedad de herramientas y recursos para construir, entrenar y desplegar modelos de aprendizaje automático, desde redes neuronales simples hasta complejas arquitecturas de aprendizaje profundo.\n",
        "\n",
        "TensorFlow utiliza una representación de datos en forma de tensores, que son estructuras de datos multidimensionales similares a matrices, y construye grafos computacionales para definir y ejecutar operaciones en estos tensores. Esto permite una gran flexibilidad y eficiencia en el desarrollo y ejecución de modelos de aprendizaje automático.\n",
        "\n",
        "Además de su versatilidad y eficiencia, TensorFlow cuenta con una amplia comunidad de desarrolladores y una extensa documentación que lo hacen muy popular en el campo del aprendizaje automático y la inteligencia artificial."
      ],
      "metadata": {
        "id": "zgEvD3NTZ-N1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **¿Qué es Hadoop?**"
      ],
      "metadata": {
        "id": "GTI8GZaHY6r5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hadoop es una estructura de software de código abierto para almacenar datos y ejecutar aplicaciones en clústeres de hardware comercial. Proporciona almacenamiento masivo para cualquier tipo de datos, enorme poder de procesamiento y la capacidad de procesar tareas o trabajos concurrentes virtualmente ilimitados.\n",
        "\n",
        "Hadoop consta de cuatro módulos principales:\n",
        "\n",
        "● Sistema de archivos distribuido de Hadoop (HDFS): un sistema de\n",
        "archivos distribuido que se ejecuta en hardware estándar o de gama\n",
        "baja. HDFS proporciona un mejor rendimiento de datos que los\n",
        "sistemas de archivos tradicionales, además de una alta tolerancia a\n",
        "fallas y soporte nativo de grandes conjuntos de datos.\n",
        "\n",
        "● Otro negociador de recursos más (YARN): administra y supervisa los\n",
        "nodos del clúster y el uso de recursos. Programa trabajos y tareas.\n",
        "\n",
        "● MapReduce: un marco que ayuda a los programas a realizar el cálculo\n",
        "paralelo de los datos. La tarea del mapa toma los datos de entrada y\n",
        "los convierte en un conjunto de datos que se puede calcular en pares\n",
        "de valores clave. La salida de la tarea del mapa se consume al reducir\n",
        "las tareas para agregar la salida y proporcionar el resultado deseado.\n",
        "Aprendizaje Esperado 1\n",
        "Fundamentos del Big Data\n",
        "\n",
        "● Hadoop Common: proporciona bibliotecas Java comunes que se\n",
        "pueden usar en todos los módulos."
      ],
      "metadata": {
        "id": "_KvJxBHPaIWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **¿Cómo funciona Hadoop?**"
      ],
      "metadata": {
        "id": "LReotmEzZB26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hadoop funciona en un entorno distribuido, lo que significa que utiliza múltiples nodos de computadora para almacenar y procesar grandes volúmenes de datos de manera eficiente. Aquí tienes una descripción básica de cómo funciona:\n",
        "\n",
        "Almacenamiento distribuido con HDFS (Hadoop Distributed File System):\n",
        "\n",
        "Los datos se dividen en bloques de tamaño fijo.\n",
        "Estos bloques se replican y se distribuyen en varios nodos del clúster para proporcionar redundancia y tolerancia a fallos.\n",
        "El NameNode actúa como un maestro que gestiona el espacio de nombres del sistema de archivos y mantiene un registro de dónde se almacenan los bloques de datos en el clúster.\n",
        "Los DataNodes son nodos de almacenamiento que almacenan los bloques de datos y responden a las solicitudes de lectura y escritura.\n",
        "Procesamiento distribuido con MapReduce:\n",
        "\n",
        "MapReduce es un modelo de programación y procesamiento distribuido que se utiliza para procesar y analizar los datos almacenados en HDFS.\n",
        "Una tarea MapReduce se divide en dos fases principales: la fase de map y la fase de reduce.\n",
        "En la fase de map, se aplican operaciones de mapeo a cada bloque de datos de entrada, generando pares clave-valor intermedios.\n",
        "Los pares clave-valor intermedios se agrupan y se envían a los nodos de reduce, donde se aplican operaciones de reducción para obtener resultados finales.\n",
        "Tanto la fase de map como la fase de reduce se ejecutan de manera distribuida en los nodos del clúster, aprovechando el paralelismo para procesar grandes volúmenes de datos de manera eficiente.\n",
        "Ejecución y administración del clúster:\n",
        "\n",
        "Un clúster Hadoop está compuesto por varios nodos que se encargan de diversas funciones, como almacenamiento, procesamiento, administración y monitoreo.\n",
        "Un administrador de clúster, como Apache YARN (Yet Another Resource Negotiator), gestiona los recursos del clúster y asigna tareas a los nodos disponibles.\n",
        "Herramientas adicionales, como Apache Ambari o Apache ZooKeeper, se utilizan para administrar y monitorear el clúster, garantizando su disponibilidad, escalabilidad y confiabilidad.\n",
        "En resumen, Hadoop funciona distribuyendo el almacenamiento y el procesamiento de datos en múltiples nodos de un clúster, lo que permite manejar grandes volúmenes de datos de manera eficiente y escalable."
      ],
      "metadata": {
        "id": "lfslSKGxiZl1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **¿Qué es una API?**"
      ],
      "metadata": {
        "id": "aDbLt44MZE_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las API son mecanismos que permiten a dos componentes de software comunicarse entre sí mediante un conjunto de definiciones y protocolos. Por ejemplo, el sistema de software del instituto de meteorología contiene datos meteorológicos diarios. La aplicación meteorológica de su teléfono “habla” con este sistema a través de las API y le muestra las actualizaciones meteorológicas diarias en su teléfono.\n",
        "\n",
        "API significa “interfaz de programación de aplicaciones”. En el contexto de las API, la palabra aplicación se refiere a cualquier software con una función distinta. La interfaz puede considerarse como un contrato de servicio entre dos aplicaciones. Este contrato define cómo se comunican entre sí mediante solicitudes y respuestas. La documentación de su API contiene información sobre cómo los desarrolladores deben estructurar esas solicitudes y respuestas.\n",
        "\n",
        "La arquitectura de las API suele explicarse en términos de cliente y servidor. La aplicación que envía la solicitud se llama cliente, y la que envía la respuesta se llama servidor. En el ejemplo del tiempo, la base de datos meteorológicos del instituto es el servidor y la aplicación móvil es el cliente."
      ],
      "metadata": {
        "id": "HFAI4ffEi5Jg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **¿Qué es Spark?**"
      ],
      "metadata": {
        "id": "smjRFZSbZIUl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apache Spark es un framework de procesamiento de datos distribuido y de código abierto diseñado para el análisis, procesamiento y manipulación de grandes volúmenes de datos de manera rápida y eficiente, ofrece velocidad, facilidad de uso, flexibilidad y escalabilidad, lo que lo hace muy popular en el campo del análisis de datos y el procesamiento de big data.\n",
        "Spark proporciona una interfaz unificada para el procesamiento de datos en diferentes tipos de datos, como datos estructurados (por ejemplo, tablas en una base de datos), datos semiestructurados (por ejemplo, archivos JSON) y datos no estructurados (por ejemplo, texto sin procesar). Utiliza una abstracción de datos llamada Resilient Distributed Dataset (RDD) para representar los datos de manera distribuida en un clúster de computadoras.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KRURH8YWjVoE"
      }
    }
  ]
}